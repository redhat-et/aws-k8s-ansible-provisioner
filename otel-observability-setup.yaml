# Run with: ansible-playbook -i inventory.ini otel-observability-setup.yaml
---
- name: Deploy OpenTelemetry Observability Stack
  hosts: all
  become: yes
  gather_facts: true
  vars:
    otel_namespace: "observability"
    llm_d_namespace: "llm-d"
    prometheus_namespace: "monitoring"  # Existing Prometheus namespace
    otel_prometheus_namespace: "otel-monitoring"  # New namespace for OTEL Prometheus
    cluster_name: "{{ ansible_hostname }}-k8s"  # Dynamic cluster name based on hostname
    
  tasks:
    - name: Create observability namespace
      shell: |
        kubectl create namespace {{ otel_namespace }} --dry-run=client -o yaml | kubectl apply -f -
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      changed_when: false

    - name: Create OpenTelemetry Operator namespace
      shell: |
        kubectl create namespace opentelemetry-operator-system --dry-run=client -o yaml | kubectl apply -f -
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      changed_when: false

    - name: Create OTEL Prometheus namespace
      shell: |
        kubectl create namespace {{ otel_prometheus_namespace }} --dry-run=client -o yaml | kubectl apply -f -
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      changed_when: false

    - name: Check if cert-manager is already installed
      shell: kubectl get pods -n cert-manager --no-headers 2>/dev/null | wc -l
      register: cert_manager_check
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      changed_when: false
      failed_when: false

    - name: Install cert-manager (required for OpenTelemetry Operator)
      shell: |
        # Install cert-manager
        kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.yaml
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: cert_manager_check.stdout | int == 0
      register: cert_manager_install

    - name: Wait for cert-manager to be ready
      shell: |
        echo "Waiting for cert-manager CRDs to be established..."
        kubectl wait --for condition=established --timeout=60s crd/certificates.cert-manager.io
        kubectl wait --for condition=established --timeout=60s crd/issuers.cert-manager.io
        kubectl wait --for condition=established --timeout=60s crd/clusterissuers.cert-manager.io
        
        echo "Waiting for cert-manager pods to be ready..."
        kubectl wait --for=condition=ready pod -l app.kubernetes.io/instance=cert-manager -n cert-manager --timeout=300s
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: cert_manager_check.stdout | int == 0
      changed_when: false

    - name: Add OpenTelemetry Helm repository
      shell: |
        helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
        helm repo update
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      changed_when: false

    - name: Install OpenTelemetry Operator
      shell: |
        helm upgrade --install opentelemetry-operator open-telemetry/opentelemetry-operator \
          --namespace opentelemetry-operator-system \
          --create-namespace \
          --set manager.collectorImage.repository=otel/opentelemetry-collector-contrib \
          --wait --timeout=10m
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: otel_operator_install

    - name: Wait for OpenTelemetry Operator CRDs to be ready
      shell: |
        echo "Waiting for OpenTelemetry CRDs to be established..."
        kubectl wait --for condition=established --timeout=60s crd/opentelemetrycollectors.opentelemetry.io
        kubectl wait --for condition=established --timeout=60s crd/instrumentations.opentelemetry.io
        
        echo "Waiting for OpenTelemetry Operator to be ready..."
        kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=opentelemetry-operator -n opentelemetry-operator-system --timeout=300s
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      changed_when: false

    - name: Create ServiceAccount for OpenTelemetry Collector
      shell: |
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: otel-collector-deployment
          namespace: {{ otel_namespace }}
        EOF
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      changed_when: false

    - name: Create ClusterRole for OpenTelemetry Collector
      shell: |
        cat <<EOF | kubectl apply -f -
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRole
        metadata:
          name: otel-collector
        rules:
        - apiGroups: [""]
          resources: ["pods", "namespaces", "nodes", "services", "endpoints"]
          verbs: ["get", "watch", "list"]
        - apiGroups: ["apps"]
          resources: ["replicasets"]
          verbs: ["get", "list", "watch"]
        - apiGroups: ["extensions"]
          resources: ["replicasets"]
          verbs: ["get", "list", "watch"]
        - apiGroups: ["config.openshift.io"]
          resources: ["infrastructures", "infrastructures/status"]
          verbs: ["get", "watch", "list"]
        - nonResourceURLs: ["/metrics"]
          verbs: ["get"]
        EOF
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      changed_when: false

    - name: Create ClusterRoleBinding for OpenTelemetry Collector
      shell: |
        cat <<EOF | kubectl apply -f -
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          name: otel-collector
        subjects:
        - kind: ServiceAccount
          name: otel-collector-deployment
          namespace: {{ otel_namespace }}
        roleRef:
          kind: ClusterRole
          name: otel-collector
          apiGroup: rbac.authorization.k8s.io
        EOF
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      changed_when: false

    - name: Add Prometheus Helm repository
      shell: |
        helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
        helm repo update
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      changed_when: false

    - name: Deploy dedicated Prometheus for OTEL
      shell: |
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: otel-prometheus-config
          namespace: {{ otel_prometheus_namespace }}
        data:
          prometheus.yml: |
            global:
              scrape_interval: 15s
              evaluation_interval: 15s
              external_labels:
                cluster: {{ cluster_name }}
                environment: "production"
                region: "local"
            
            scrape_configs:
              - job_name: 'prometheus'
                static_configs:
                  - targets: ['localhost:9090']
                    labels:
                      cluster: {{ cluster_name }}
            
            remote_write:
              - url: http://localhost:9090/api/v1/write
                queue_config:
                  max_samples_per_send: 1000
                  max_shards: 200
                  capacity: 2500
        ---
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: otel-prometheus
          namespace: {{ otel_prometheus_namespace }}
          labels:
            app: otel-prometheus
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: otel-prometheus
          template:
            metadata:
              labels:
                app: otel-prometheus
            spec:
              containers:
              - name: prometheus
                image: prom/prometheus:v2.47.0
                args:
                  - '--config.file=/etc/prometheus/prometheus.yml'
                  - '--storage.tsdb.path=/prometheus/'
                  - '--web.console.libraries=/etc/prometheus/console_libraries'
                  - '--web.console.templates=/etc/prometheus/consoles'
                  - '--storage.tsdb.retention.time=30d'
                  - '--storage.tsdb.retention.size=10GB'
                  - '--web.enable-lifecycle'
                  - '--web.enable-remote-write-receiver'
                  - '--enable-feature=remote-write-receiver'
                ports:
                - containerPort: 9090
                  name: web
                volumeMounts:
                - name: prometheus-config
                  mountPath: /etc/prometheus
                - name: prometheus-storage
                  mountPath: /prometheus
                resources:
                  requests:
                    memory: "1Gi"
                    cpu: "500m"
                  limits:
                    memory: "2Gi"
                    cpu: "1000m"
              volumes:
              - name: prometheus-config
                configMap:
                  name: otel-prometheus-config
              - name: prometheus-storage
                emptyDir:
                  sizeLimit: 10Gi
        ---
        apiVersion: v1
        kind: Service
        metadata:
          name: otel-prometheus
          namespace: {{ otel_prometheus_namespace }}
          labels:
            app: otel-prometheus
        spec:
          selector:
            app: otel-prometheus
          ports:
          - name: web
            port: 9090
            targetPort: 9090
          type: ClusterIP
        EOF
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      changed_when: false

    - name: Wait for OTEL Prometheus to be ready
      shell: |
        echo "Waiting for OTEL Prometheus to be ready..."
        kubectl wait --for=condition=ready pod \
          -l app=otel-prometheus \
          -n {{ otel_prometheus_namespace }} \
          --timeout=300s
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      changed_when: false

    - name: Deploy OpenTelemetry Collector for metrics collection with cluster identification
      shell: |
        cat <<EOF | kubectl apply -f -
        apiVersion: opentelemetry.io/v1beta1
        kind: OpenTelemetryCollector
        metadata:
          name: otel-metrics-collector
          namespace: {{ otel_namespace }}
        spec:
          mode: daemonset
          serviceAccount: otel-collector-deployment
          env:
            - name: CLUSTER_NAME
              value: "{{ cluster_name }}"
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          config:
            receivers:
              # Prometheus receiver for scraping vLLM and DCGM metrics
              # Updated configuration to properly discover and collect:
              # 1. vLLM metrics from llm-d namespace with enhanced labeling
              # 2. DCGM GPU metrics from gpu-operator namespace (correct location)
              # 3. Both service and pod discovery for DCGM exporter for reliability
              prometheus:
                config:
                  scrape_configs:
                    - job_name: 'vllm-metrics'
                      kubernetes_sd_configs:
                        - role: pod
                          namespaces:
                            names:
                              - {{ llm_d_namespace }}
                      relabel_configs:
                        # Keep only pods with prometheus.io/scrape annotation set to true
                        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                          action: keep
                          regex: true
                        # Use custom metrics path if specified, otherwise default to /metrics
                        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                          action: replace
                          target_label: __metrics_path__
                          regex: (.+)
                        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                          action: replace
                          target_label: __metrics_path__
                          regex: ^$
                          replacement: /metrics
                        # Use custom port if specified, otherwise default to 8000
                        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                          action: replace
                          regex: ([^:]+)(?::\d+)?;(\d+)
                          replacement: \$1:\$2
                          target_label: __address__
                        - source_labels: [__address__]
                          action: replace
                          regex: ([^:]+)(?::\d+)?
                          replacement: \$1:8000
                          target_label: __address__
                        # Map pod labels to metric labels
                        - action: labelmap
                          regex: __meta_kubernetes_pod_label_(.+)
                        # Add Kubernetes metadata
                        - source_labels: [__meta_kubernetes_namespace]
                          action: replace
                          target_label: kubernetes_namespace
                        - source_labels: [__meta_kubernetes_pod_name]
                          action: replace
                          target_label: kubernetes_pod_name
                        - source_labels: [__meta_kubernetes_pod_node_name]
                          action: replace
                          target_label: kubernetes_node_name
                        # Add cluster name to all scraped metrics
                        - target_label: cluster_name
                          replacement: {{ cluster_name }}
                        - target_label: cluster
                          replacement: {{ cluster_name }}
                        # Add service identification for vLLM
                        - target_label: service
                          replacement: vllm
                        - target_label: job_type
                          replacement: llm-inference
                    
                    - job_name: 'existing-dcgm-exporter'
                      kubernetes_sd_configs:
                        - role: service
                          namespaces:
                            names:
                              - gpu-operator
                      relabel_configs:
                        # Keep only nvidia-dcgm-exporter services
                        - source_labels: [__meta_kubernetes_service_label_app]
                          action: keep
                          regex: nvidia-dcgm-exporter
                        # Use the gpu-metrics port name instead of hardcoded port
                        - source_labels: [__meta_kubernetes_service_port_name]
                          action: keep
                          regex: gpu-metrics
                        # Replace address with the discovered port (instead of hardcoding 9400)
                        - source_labels: [__address__]
                          action: replace
                          target_label: __address__
                        - action: labelmap
                          regex: __meta_kubernetes_service_label_(.+)
                        - source_labels: [__meta_kubernetes_namespace]
                          action: replace
                          target_label: kubernetes_namespace
                        - source_labels: [__meta_kubernetes_service_name]
                          action: replace
                          target_label: kubernetes_service_name
                        # Add cluster name to all scraped metrics
                        - target_label: cluster_name
                          replacement: {{ cluster_name }}
                        - target_label: cluster
                          replacement: {{ cluster_name }}
                    
                    # Additional job to discover DCGM metrics via pod discovery (backup method)
                    - job_name: 'dcgm-exporter-pods'
                      kubernetes_sd_configs:
                        - role: pod
                          namespaces:
                            names:
                              - gpu-operator
                      relabel_configs:
                        # Keep only pods with the nvidia-dcgm-exporter app label
                        - source_labels: [__meta_kubernetes_pod_label_app]
                          action: keep
                          regex: nvidia-dcgm-exporter
                        # Look for the gpu-metrics port in the pod spec
                        - source_labels: [__meta_kubernetes_pod_container_port_name]
                          action: keep
                          regex: gpu-metrics
                        # Use prometheus.io annotations if present, otherwise use default path
                        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                          action: replace
                          target_label: __metrics_path__
                          regex: (.+)
                        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                          action: replace
                          target_label: __metrics_path__
                          regex: ^$
                          replacement: /metrics
                        - action: labelmap
                          regex: __meta_kubernetes_pod_label_(.+)
                        - source_labels: [__meta_kubernetes_namespace]
                          action: replace
                          target_label: kubernetes_namespace
                        - source_labels: [__meta_kubernetes_pod_name]
                          action: replace
                          target_label: kubernetes_pod_name
                        # Add node name as instance (matching the ServiceMonitor behavior)
                        - source_labels: [__meta_kubernetes_pod_node_name]
                          action: replace
                          target_label: instance
                        # Add cluster name to all scraped metrics
                        - target_label: cluster_name
                          replacement: {{ cluster_name }}
                        - target_label: cluster
                          replacement: {{ cluster_name }}
                    
                    # Scrape Kubernetes system metrics with cluster identification
                    - job_name: 'kubernetes-nodes'
                      kubernetes_sd_configs:
                        - role: node
                      relabel_configs:
                        - action: labelmap
                          regex: __meta_kubernetes_node_label_(.+)
                        - target_label: __address__
                          replacement: kubernetes.default.svc:443
                        - source_labels: [__meta_kubernetes_node_name]
                          regex: (.+)
                          target_label: __metrics_path__
                          replacement: /api/v1/nodes/\$1/proxy/metrics
                        # Add cluster name to all scraped metrics
                        - target_label: cluster_name
                          replacement: {{ cluster_name }}
                    
                    - job_name: 'kubernetes-cadvisor'
                      kubernetes_sd_configs:
                        - role: node
                      relabel_configs:
                        - action: labelmap
                          regex: __meta_kubernetes_node_label_(.+)
                        - target_label: __address__
                          replacement: kubernetes.default.svc:443
                        - source_labels: [__meta_kubernetes_node_name]
                          regex: (.+)
                          target_label: __metrics_path__
                          replacement: /api/v1/nodes/\$1/proxy/metrics/cadvisor
                        # Add cluster name to all scraped metrics
                        - target_label: cluster_name
                          replacement: {{ cluster_name }}
              
              # OTLP receiver for traces and metrics
              otlp:
                protocols:
                  grpc:
                    endpoint: 0.0.0.0:4317
                  http:
                    endpoint: 0.0.0.0:4318
            
            processors:
              # Batch processor
              batch:
                timeout: 1s
                send_batch_size: 1024
              
              # Memory limiter
              memory_limiter:
                check_interval: 1s
                limit_percentage: 50
                spike_limit_percentage: 30
              
              # Resource processor to add cluster information
              resource:
                attributes:
                  - key: cluster.name
                    value: {{ cluster_name }}
                    action: upsert
                  - key: cluster.environment
                    value: "production"  # Change this as needed
                    action: upsert
                  - key: cluster.region
                    value: "local"  # Change this as needed
                    action: upsert
                  - key: k8s.cluster.name
                    value: {{ cluster_name }}
                    action: upsert
                  - key: deployment.environment
                    value: "k8s-single-node"
                    action: upsert
              
              # Metrics transform processor to ensure cluster labels
              metricstransform:
                transforms:
                  - include: ".*"
                    match_type: regexp
                    action: update
                    operations:
                      - action: add_label
                        new_label: cluster_name
                        new_value: {{ cluster_name }}
                      - action: add_label
                        new_label: cluster
                        new_value: {{ cluster_name }}
              
              # Kubernetes attributes processor with enhanced cluster metadata
              k8sattributes:
                auth_type: "serviceAccount"
                passthrough: false
                extract:
                  metadata:
                    - k8s.pod.name
                    - k8s.pod.uid
                    - k8s.deployment.name
                    - k8s.namespace.name
                    - k8s.node.name
                    - k8s.pod.start_time
                  labels:
                    - tag_name: k8s.pod.label.app
                      key: app
                      from: pod
                    - tag_name: k8s.pod.label.version
                      key: version
                      from: pod
                  annotations:
                    - tag_name: k8s.pod.annotation.prometheus.io/scrape
                      key: prometheus.io/scrape
                      from: pod
                pod_association:
                  - sources:
                    - from: resource_attribute
                      name: k8s.pod.ip
                  - sources:
                    - from: resource_attribute
                      name: k8s.pod.uid
                  - sources:
                    - from: connection
              
              # Enhanced resource detection processor
              resourcedetection:
                detectors: [env, system, k8snode, docker]
                timeout: 5s
                override: false
                system:
                  hostname_sources: [os]
                k8snode:
                  auth_type: serviceAccount
                  node_from_env_var: K8S_NODE_NAME
            
            exporters:
              # Prometheus remote write exporter - using dedicated OTEL Prometheus
              prometheusremotewrite:
                endpoint: "http://otel-prometheus.{{ otel_prometheus_namespace }}.svc.cluster.local:9090/api/v1/write"
                tls:
                  insecure: true
                external_labels:
                  cluster: {{ cluster_name }}
                  environment: "production"
                  region: "local"
                  collector_type: "otel"
                headers:
                  X-Prometheus-Remote-Write-Version: "0.1.0"
              
              # OTLP exporter (for sending to other collectors if needed)
              otlp:
                endpoint: "http://localhost:4317"
                tls:
                  insecure: true
              
              # Debug exporter for debugging (replaces deprecated logging exporter)
              debug:
                verbosity: normal
                sampling_initial: 5
                sampling_thereafter: 200
            
            service:
              pipelines:
                metrics:
                  receivers: [prometheus, otlp]
                  processors: [memory_limiter, resource, metricstransform, k8sattributes, resourcedetection, batch]
                  exporters: [prometheusremotewrite, debug]
                
                traces:
                  receivers: [otlp]
                  processors: [memory_limiter, resource, k8sattributes, resourcedetection, batch]
                  exporters: [debug]
        EOF
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: otel_collector_deploy
      changed_when: false

    - name: Wait for OpenTelemetry components to be ready
      shell: |
        echo "Waiting for OpenTelemetry Collector to be ready..."
        kubectl wait --for=condition=ready pod \
          -l app.kubernetes.io/name=otel-metrics-collector \
          -n {{ otel_namespace }} \
          --timeout=300s || true
        
        echo "Checking OTEL Prometheus status..."
        kubectl get pods -n {{ otel_prometheus_namespace }} -l app=otel-prometheus
        
        echo "Checking existing DCGM exporter status..."
        echo "Looking for DCGM exporter in gpu-operator namespace (correct location):"
        kubectl get pods -n gpu-operator -l app=nvidia-dcgm-exporter || echo "DCGM exporter not found in gpu-operator namespace"
        kubectl get services -n gpu-operator -l app=nvidia-dcgm-exporter || echo "DCGM exporter service not found in gpu-operator namespace"
        
        echo "Checking vLLM pods in {{ llm_d_namespace }} namespace:"
        kubectl get pods -n {{ llm_d_namespace }} || echo "No pods found in {{ llm_d_namespace }} namespace"
        kubectl get pods -n {{ llm_d_namespace }} -o jsonpath='{.items[*].metadata.annotations}' | grep -i prometheus || echo "No prometheus annotations found on vLLM pods"
        
        echo "Checking existing Prometheus status..."
        kubectl get pods -n {{ prometheus_namespace }} -l app.kubernetes.io/name=prometheus
        
        echo "Checking existing Grafana status..."
        kubectl get pods -n {{ prometheus_namespace }} -l app.kubernetes.io/name=grafana
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: wait_result
      changed_when: false

    - name: Display OTEL Prometheus connection information
      debug:
        msg: |
          ========================================
          OTEL Prometheus Instance Deployed Successfully!
          ========================================
          
          Namespace: {{ otel_prometheus_namespace }}
          Service: otel-prometheus.{{ otel_prometheus_namespace }}.svc.cluster.local:9090
          Cluster Name: {{ cluster_name }}
          
          To access the OTEL Prometheus UI:
          kubectl port-forward -n {{ otel_prometheus_namespace }} svc/otel-prometheus 9091:9090
          
          Then open: http://localhost:9091
          
          All metrics collected by OTEL will include cluster identification:
          - cluster_name: {{ cluster_name }}
          - cluster.name: {{ cluster_name }}
          - k8s.cluster.name: {{ cluster_name }}
          
          This allows you to build multi-cluster dashboards and filter by cluster.
          ========================================

    - name: Verify cluster labels are working (run after collector is ready)
      shell: |
        echo "Waiting for OTEL collector to start collecting metrics..."
        sleep 30
        
        echo "Testing cluster identification in OTEL Prometheus..."
        # Port forward in background for testing
        kubectl port-forward -n {{ otel_prometheus_namespace }} svc/otel-prometheus 9091:9090 &
        PF_PID=$!
        sleep 10
        
        echo "=== Cluster Labels Verification ==="
        echo "Cluster Name: {{ cluster_name }}"
        echo ""
        
        # Test for cluster labels
        echo "1. Checking for cluster labels in metrics:"
        curl -s "http://localhost:9091/api/v1/label/cluster/values" 2>/dev/null | jq -r '.data[]' 2>/dev/null || echo "No cluster labels found yet (may need more time)"
        
        echo ""
        echo "2. Checking for cluster_name labels:"
        curl -s "http://localhost:9091/api/v1/label/cluster_name/values" 2>/dev/null | jq -r '.data[]' 2>/dev/null || echo "No cluster_name labels found yet (may need more time)"
        
        echo ""
        echo "3. Sample metric with cluster filtering:"
        curl -s "http://localhost:9091/api/v1/query?query=up{cluster=\"{{ cluster_name }}\"}" 2>/dev/null | jq -r '.data.result | length' 2>/dev/null || echo "0"
        
        echo ""
        echo "4. Checking for vLLM metrics:"
        vllm_metrics=$(curl -s "http://localhost:9091/api/v1/query?query=vllm_request_total" 2>/dev/null | jq -r '.data.result | length' 2>/dev/null || echo "0")
        echo "vLLM metrics found: $vllm_metrics"
        if [ "$vllm_metrics" = "0" ]; then
          echo "Note: vLLM metrics not found. This is normal if vLLM pods are not yet deployed or don't have prometheus.io/scrape=true annotation"
        fi
        
        echo ""
        echo "5. Checking for DCGM metrics:"
        dcgm_metrics=$(curl -s "http://localhost:9091/api/v1/query?query=DCGM_FI_DEV_GPU_UTIL" 2>/dev/null | jq -r '.data.result | length' 2>/dev/null || echo "0")
        echo "DCGM metrics found: $dcgm_metrics"
        if [ "$dcgm_metrics" = "0" ]; then
          echo "Note: DCGM metrics not found. This may indicate:"
          echo "  - GPU Operator not fully deployed yet"
          echo "  - DCGM exporter not accessible in gpu-operator namespace"
          echo "  - Need to wait for GPU Operator installation to complete"
        fi
        
        echo ""
        echo "6. Available metrics summary:"
        total_metrics=$(curl -s "http://localhost:9091/api/v1/label/__name__/values" 2>/dev/null | jq -r '.data | length' 2>/dev/null || echo "0")
        echo "Total unique metrics in OTEL Prometheus: $total_metrics"
        
        # Cleanup
        kill $PF_PID 2>/dev/null || true
        
        echo ""
        echo "=== Multi-Cluster Query Examples ==="
        echo "Use these queries in Grafana to filter by cluster:"
        echo "- All metrics from this cluster: {cluster=\"{{ cluster_name }}\"}"
        echo ""
        echo "vLLM Metrics (if vLLM pods are running):"
        echo "- Request rate: rate(vllm_request_total{cluster=\"{{ cluster_name }}\"}[5m])"
        echo "- Active requests: vllm_active_requests{cluster=\"{{ cluster_name }}\"}"
        echo "- Request duration: histogram_quantile(0.95, rate(vllm_request_duration_seconds_bucket{cluster=\"{{ cluster_name }}\"}[5m]))"
        echo ""
        echo "DCGM GPU Metrics (if GPU Operator is running):"
        echo "- GPU utilization: DCGM_FI_DEV_GPU_UTIL{cluster=\"{{ cluster_name }}\"}"
        echo "- GPU memory utilization: DCGM_FI_DEV_MEM_COPY_UTIL{cluster=\"{{ cluster_name }}\"}"
        echo "- GPU temperature: DCGM_FI_DEV_GPU_TEMP{cluster=\"{{ cluster_name }}\"}"
        echo "- GPU power usage: DCGM_FI_DEV_POWER_USAGE{cluster=\"{{ cluster_name }}\"}"
        echo ""
        echo "OpenTelemetry Collector Health:"
        echo "- Metrics processing: rate(otelcol_processor_batch_batch_send_size_sum{cluster=\"{{ cluster_name }}\"}[5m])"
        echo "- Metrics received: rate(otelcol_receiver_accepted_metric_points_total{cluster=\"{{ cluster_name }}\"}[5m])"
        echo ""
        echo "=== Grafana Dashboard Variables ==="
        echo "Create a 'cluster' variable in Grafana with query: label_values(cluster)"
        echo "Then use \$cluster in your queries for dynamic cluster selection!"
        
      become: no
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: cluster_verification
      changed_when: false
